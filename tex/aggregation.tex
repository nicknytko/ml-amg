\documentclass{article}

\usepackage{lipsum}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{algorithmic}
\ifpdf%
  \DeclareGraphicsExtensions{.eps,.pdf,.png,.jpg}
\else
  \DeclareGraphicsExtensions{.eps}
\fi
\usepackage{amsopn}
\DeclareMathOperator{\diag}{diag}
\usepackage{booktabs}
\usepackage{bbm}
\usepackage{bm}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[margin=1.5in]{geometry}
\usepackage{hyperref}
\usepackage[pdf]{graphviz}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\normtwo}[1]{\left\lVert#1\right\rVert_2}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\mat}[1]{\bm{{#1}}}
\renewcommand{\vec}[1]{\bm{{#1}}}
\newcommand{\lequiv}{\Leftrightarrow}
\newcommand{\bigO}[1]{\mathcal{O}\!\left(#1\right)}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\sfrac}[2]{#1/#2}
\newcommand{\hquad}{\enskip}
\newcommand{\expected}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\mspan}[1]{\text{span}\left( #1 \right)}
\newcommand{\prob}[1]{P\left(#1\right)}
\newcommand{\probt}[1]{P\left( \text{#1} \right)}
\newcommand{\condprob}[2]{P\left(#1 \:|\: #2\right)}
\newcommand{\condprobt}[2]{P\left(\text{#1} \:|\: \text{#2}\right)}
\newcommand{\bayes}[2]{\frac{\condprob{#2}{#1}\prob{#1}}{\prob{#2}}}
\newcommand{\bayesx}[3]{\frac{\condprob{#2}{#1}\prob{#1}}{\condprob{#2}{#1}\prob{#1} + \condprob{#2}{#3}\prob{#3}}}
\newcommand{\sech}{\text{sech}}
\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}
\newcommand{\vect}[2]{\underline{{#1}}_{{#2}}}
\newcommand{\basisp}[1]{\underline{{p}}_{{#1}}}
\newcommand{\basisq}[1]{\underline{{q}}_{{#1}}}
\newcommand{\coeff}[1]{\underline{{a}}_{{#1}}}
\newcommand{\bestfit}{\underline{\bar{x}}}
\newcommand{\grad}{\nabla}
\newcommand{\laplace}{\Delta}
\newcommand{\setbar}{\:\middle|\:}
\renewcommand{\div}{\grad \cdot}
\renewcommand{\Re}{\text{Re}}

\begin{document}
\section{Background}\label{sec:background}
Currently, we are attempting to learn an interpolation operator $\mat{P}$ given a pre-existing aggregation of some system $\mat{A}$.  The graphnet being used is a combination of \textit{node convolution} and \textit{edge convolution} layers.  The main distinction between the two is that the node convolution accepts some nodal values $\vec{v}$, and outputs a new set of nodal values $\vec{v}^\prime$.  The edge convolution accepts edge attributes $\vec{e}$ and outputs $\vec{e}^\prime$.  We can think of this as modifying the values (but not the structure) of the input graph at each layer.

Because of this, the graphnet takes as input some graph $\mat{G} = \left(\vec{v}, \vec{e}\right)$ and outputs a new graph $\mat{G}^\prime = \left(\vec{v}^\prime, \vec{e}^\prime\right)$.  The graphs $\mat{G}$ and $\mat{G}^\prime$ have the same underlying structure and node connectivity, but may contain differing nodal and edge weights (including a different number of attributes per node/edge).

We can extend the above to work analogously with sparse matrices.  If we have some sparse matrix $\mat{A}$, we can create the graph $\mat{G}_{\mat{A}}$ by outputting one node per row/column and assigning edges based on nonzero entries of $\mat{A}_{ij}$.  Therefore, if we were to generate the graph $\mat{G}_{\mat{A}}$ based on $\mat{A}$, we will get some output graph $\mat{G}_{\mat{A}}^\prime$ that can be transformed back into a sparse matrix $\mat{A}^\prime$.  Because the graph structure is unchanged, we must have that $\mat{A}^\prime$ contains the same sparsity as $\mat{A}$, though the nonzero entries may be different.

This detail prevents us from inputting some matrix $\mat{A}$ to the graphnet and directly obtaining a $\mat{P}$ back; we are constrained by the sparsity we are able to obtain.  If we convolve on some matrix containing the sparsity of $\mat{P}$ instead, we may lose information that was present in $\mat{A}$.  In section \ref{sec:graphnet}, we will describe the network, inputs, and outputs in more detail.  In section \ref{sec:avg_pooling}, we will explore how given some $\mat{A}^\prime$ from the output of the graphnet, we can transform it into a suitable $\mat{P}$ that can be used in an AMG method.

\section{Network Details}\label{sec:graphnet}
The matrix $\mat{A}$ is first turned into a graph $\mat{G}_{\mat{A}}$ by the description above; each row/column is converted into a node $n_i$ and non-zero entries of $\abs{\mat{A}_{ij}}$ are converted into edges between nodes $i$ and $j$.  We then add an additional binary edge attribute $\theta_{ij}$, given by
\begin{equation}
  \theta_{ij} = \begin{cases} 1 & \text{nodes } i,j \text{ in same aggregate} \\ 0 & \text{otherwise} \end{cases}.
\end{equation}
This means that each edge has two attributes: $\abs{\mat{A}_{ij}}$ and the binary feature from above.  The feature $\theta_{ij}$ is how the graphnet is ``aware'' of the aggregate information.

The network is defined as three message-passing node-convolution layers, followed by one edge-convolution layer.  This takes as input a graph $\mat{G} = \left(\vec{v}, \vec{e}\right)$ and gives a new graph $\mat{G}^\prime = \left(\vec{v}^\prime, \vec{e}\right)$ with the same connectivity information.  Of course, we are only interested in the edge values; the nodal values are a by-product used in intermediate convolutions.  So, given a sparse matrix $\mat{A}$, the graphnet will output a new sparse matrix $\mat{A}^\prime$ with the same sparsity pattern.

\section{Aggregation-Based Average Pooling}\label{sec:avg_pooling}
This simple approach works by ``collapsing'' columns of nodes in similar aggregates, forming a $\mat{P}$ that contains a reasonable sparsity pattern for aggregation-based AMG.



Denote $Agg$ the tentative aggregation operator without smoothing, for example,
\begin{equation}
Agg =
\begin{bmatrix}
  1 & \\
  1 & \\
  1 & \\
  & 1 \\
  & 1
\end{bmatrix},
\end{equation}
would correspond to a $5 \times 5$ system with two aggregates, the first containing nodes $1-3$ and the second containing nodes $4$ and $5$.  Let $\mat{A}$ be the system being solved and $\mat{P}$ the interpolation operator, i.e. that obtained by running some relaxation scheme on $Agg$.

One way to solve this is to first form $\mat{\hat{P}}$ as usual using the graphnet's node and edge convolutions.  This will give us a matrix that has the same sparsity pattern as $\mat{A}$.  Continuing the example from above, we may obtain some $\mat{\hat{P}}$ like
\begin{equation}
\mat{\hat{P}} =
\left[\begin{array}{@{}ccc|cc@{}}
  \sfrac{1}{2} & \sfrac{1}{3} & & & \\
  \sfrac{1}{2} & \sfrac{1}{3} & \sfrac{1}{3} & & \\
               & \sfrac{1}{3} & \sfrac{1}{3} & \sfrac{1}{3} &\\
               &              & \sfrac{1}{3} & \sfrac{1}{3} & \sfrac{1}{2} \\
  &              &              & \sfrac{1}{3} & \sfrac{1}{2}
\end{array}\right],
\end{equation}
with nodes to the left of the bar signifying those belonging to the first aggregate, and those to the right belonging to the second aggregate.

Then, we form $\hat{Agg}$, which is the $Agg$ operator from above except with its columns normalized in the $1$-norm.
\begin{equation}
\hat{Agg} =
\begin{bmatrix}
  \sfrac{1}{3} & \\
  \sfrac{1}{3} & \\
  \sfrac{1}{3} & \\
  & \sfrac{1}{2} \\
  & \sfrac{1}{2}
\end{bmatrix}.
\end{equation}
Now, we can ``collapse'' the columns of the nodes corresponding to each aggregate by averaging them together.  This can be written concretely as
\begin{equation}
\mat{P} = \mat{\hat{P}}\hat{Agg} = \begin{bmatrix}
  0.277 & \\
  0.333 & \\
  0.222 & 0.166 \\
  0.111 & 0.416 \\
  & 0.416 \\
\end{bmatrix},
\end{equation}
which gives a reasonable sparsity pattern for the interpolation, given $Agg$.  Note that if we view this as a graph, we get edge connections between aggregates ``for free'', which is exactly what is wanted.

\section{Problem Setup}
The specific problem being solved is a diffusion problem with Neumann boundary conditions.  To test the feasibility of the method, results were fixed for one problem and the networks/methods allowed to ``overfit'' to see if some reasonable solution is obtained.  Aggregates were generated with Lloyd aggregation, using a seed ratio of $0.25$.  This gives a relatively high number of small aggregates, which may be somewhat unwanted in practice.

\begin{figure}[h]
  \includegraphics[width=\textwidth]{figures/diffusion-lloyd-aggregates.pdf}
  \caption{Specific problem being solved, with aggregate information.}
  \label{fig:lloyd_aggregates}
\end{figure}

\section{Numerical Results}
Using Lloyd to generate the aggregates and then generating $\mat{P}$ with a Jacobi prolongation smoother results in an AMG method with a convergence factor of approx. $0.6874$, as measured by the loss function.  This will be used as a baseline to compare the following results.

All of the following plots are using the same random seeds, meaning both Lloyd aggregates and test vectors for the AMG loss should remain fixed for each training iteration.

\begin{figure}[h]
  \includegraphics[width=\textwidth]{figures/learn_p_directly.pdf}
  \caption{Loss function when directly optimizing the nonzero entries of $\mat{P}$.  Using Adam optimizer with a learning rate of $0.01$.}
  \label{fig:loss_p}
\end{figure}

\begin{figure}[h]
  \includegraphics[width=\textwidth]{figures/learn_p_hat.pdf}
  \caption{Loss function when optimizing the nonzero entries of $\mat{\hat{P}}$, then forming $\mat{P}=\mat{\hat{P}}\hat{Agg}$.  Using Adam optimizer with a learning rate of $0.01$.}
  \label{fig:loss_p_hat}
\end{figure}

\begin{figure}[h]
  \includegraphics[width=\textwidth]{figures/learn_graphnet.pdf}
  \caption{Loss function when using a Graphnet to generate $\mat{\hat{P}}$.  Using Adam optimizer with a learning rate of $10^{-5}$.}
  \label{fig:loss_graphnet}
\end{figure}

\end{document}
